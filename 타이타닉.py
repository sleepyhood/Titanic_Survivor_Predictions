# -*- coding: utf-8 -*-
"""타이타닉.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10GudfWXcezmJs6-VIild7vv3v1Gpq9ev


데이터 출처: https://www.kaggle.com/c/titanic/data
#[1] 필요한 라이브러리 호출
"""

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns

"""#[2] train set과 test set를 불러온 후, 5개의 행을 확인"""

train = pd.read_csv("./train.csv")
test = pd.read_csv("./test.csv")
train.head()
test.head()

"""#[3] train set의 컬럼들이 무엇이 있는지 확인"""

train.columns.unique

"""#[4] 결측치 확인"""

train.info()

"""#[5] 이상치 확인"""

train.describe()

del train["PassengerId"]

train.columns.unique

test.drop("PassengerId", axis=1, inplace=True)
test.columns.unique

# y_train = train['Survived']
# train.drop('Survived', axis=1, inplace=True)
# train.head()

train.shape

test.shape

"""# 결측값 채우기

* train, test: age, cabin

* train: Embarked

* test: Fare

과 같이 해당 컬럼에 결측값이 존재
"""

train.info()

train["Embarked"]

"""## train['Embarked']의 결측값을 확인하기 위해, 값들의 분포를 정규화로 확인

"""

train["Embarked"].value_counts(normalize=True)

"""train['Embarked']의 결측값을 값들 중 가장 많이 분포된 S로 채움"""

train["Embarked"].fillna("S", inplace=True)

train.info()

"""

---

"""

test.info()

test["Fare"].value_counts()

train["Fare"].describe()

"""test['Fare'] 항목을 중앙값(50%)으로 채움"""

test["Fare"].fillna("14", inplace=True)
test.info()

train.corr()

train["Pclass"].value_counts()

train["Age"].value_counts()

train.groupby(train["Pclass"]).mean()

"""train의 Embarked 컬럼 별로 train의 Pclass 컬럼의 평균 값을 보여줌."""

train["Pclass"].groupby(train["Embarked"]).agg("mean")
# train['Pclass'].groupby(train["Embarked"]).mean()

"""Pclass, Sex 컬럼 별로 train 내 전체 데이터의 평균값을 보여줌."""

train.groupby(["Pclass", "Sex"]).mean()

"""Pclass, Sex 컬럼 별로 trian의 Pclass, Sex, Age 컬럼의 평균을 보여줌."""

train[["Pclass", "Sex", "Age"]].groupby(["Pclass", "Sex"]).mean()

"""객실과 성별 별로 나이의 중앙값

중앙값을 결측치에 채우기 위해 age_table 에 담는다.
"""

age_table = train[["Pclass", "Sex", "Age"]].groupby(["Pclass", "Sex"]).median()
age_table

# Series 형태
age_table.loc[3, "female"][0]

"""* 데이터 값이 결측치인지 아닌지 확인해주는 함수
* 결측치를 채우는 함수를 만든다.
* 결측치가 있다면 age_table 데이터 값으로 채워준다.

"""

train.iloc[5]


def fill_age(person):
    if np.isnan(person["Age"]):
        return age_table.loc[person["Pclass"], person["Sex"]][0]
    else:
        return person["Age"]


# 5번째 행의 age 는 null 이다.
np.isnan(train.iloc[5]["Age"])

# apply : 한 줄씩 입출력하는 것
train["Age"] = train.apply(fill_age, axis=1)
test["Age"] = test.apply(fill_age, axis=1)

train.info()

train["Cabin"]

train["Cabin"].fillna("M", inplace=True)
test["Cabin"].fillna("M", inplace=True)

train.info()

"""EDA 탐색적 데이터 분석"""

test["Family_size"] = test["SibSp"] + test["Parch"] + 1
train["Family_size"] = train["SibSp"] + train["Parch"] + 1

# x 축 하나에 비교값이 여러 개 가능 (hue == 구분자)

sns.countplot(data=train, x="Family_size", hue="Survived")

sns.countplot(data=train, x="Embarked", hue="Survived")

sns.violinplot(data=train, x="Sex", y="Age", hue="Survived", split=True)

sns.violinplot(data=train, x="Sex", y="Fare", hue="Survived", split=True)

sns.lineplot(data=train, x="Pclass", y="Survived")

# 주의: Cabin, Name, Ticket 열은 범주화 조차도 불필요
# categorical_feature = ['Sex', 'Cabin', 'Embarked', 'Ticket', 'Name']
categorical_feature = ["Sex", "Embarked"]

y_train = train["Survived"]

train.drop("Survived", axis=1, inplace=True)

# concat : 행, 열을 기준으로 데이터프레임을 합침.
# merge : 데이터를 기준으로 데이터프레임을 합침.
combine = pd.concat([train, test], ignore_index=True)
combine.head()

one_hot = pd.get_dummies(combine[categorical_feature])
one_hot.head()

combine.drop(categorical_feature, axis=1, inplace=True)
combine.head()

total_combine = pd.concat([combine, one_hot], axis=1)

total_combine.head()

#################################################
from sklearn.tree import DecisionTreeClassifier  # 분류

tree_model = DecisionTreeClassifier()

total_combine = total_combine.drop(["Name", "Ticket", "Cabin"], axis=1)

X_train = total_combine.iloc[:891]
X_test = total_combine.iloc[891:]


print(X_train.shape)
print(X_test.shape)
print(y_train.shape)

tree_model.fit(X_train, y_train)
DecisionTreeClassifier()

######################
## CSV 쓰기
#######################
pre = tree_model.predict(X_test)
gender = pd.read_csv("./gender_submission.csv")

# gender DataFrame 에 Survived 라는 컬럼으로 pre 값 저장
gender["Survived"] = pre
gender.to_csv("mySubmission01.csv", index=False)


###################
# 분류 모델 시각화
###################

# X_train에서 'Name', 'Ticket', 'Cabin' 열 제외
# X_train2 = X_train.drop(["Name", "Ticket", "Cabin"], axis=1)
X_train2 = X_train

# y_train은 'Survived' 열
# 'Survived' 열을 범주형으로 변환
y_train2 = y_train.astype("category")


# 'Sex'와 'Embarked' 열에 대해 One-Hot Encoding
# X_train2 = pd.get_dummies(X_train2, columns=['Sex', 'Embarked'])

from sklearn.model_selection import train_test_split

# train_test_split을 사용하여 데이터를 학습용과 검증용으로 나눔
X_train2, X_test2, y_train2, y_test2 = train_test_split(
    X_train2, y_train2, test_size=0.2, random_state=42
)

import matplotlib

from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import AdaBoostClassifier
import matplotlib.pyplot as plt

# 분류 모델 초기화
models = [
    RandomForestClassifier(),
    GradientBoostingClassifier(),
    KNeighborsClassifier(),
    GaussianNB(),
    AdaBoostClassifier(),
    SVC(),
    DecisionTreeClassifier(),
    # LogisticRegression(random_state=42, solver="lbfgs", max_iter=100),
]

# 색상 정의
colors = ["blue", "orange", "green", "red", "purple", "brown", "pink", "gray"]

# 정확도를 저장할 리스트
accuracies = []

# 각 모델에 대해 학습 및 예측 수행
for i, model in enumerate(models):
    model.fit(X_train2, y_train2)
    y_pred = model.predict(X_test2)
    accuracy = accuracy_score(y_test2, y_pred)
    accuracies.append(accuracy)

# 정확도를 기준으로 내림차순으로 정렬
# accuracies.sort(key=lambda x: x[1], reverse=True)

# 현재 열려있는 모든 플롯 해제
plt.close("all")

# 정확도를 시각화
plt.bar([str(model).split("(")[0] for model in models], accuracies, color=colors)
plt.xlabel("Model")
plt.ylabel("Accuracy")
plt.title("Accuracy by classification model")

# X 축의 글자를 10도 기울임
plt.xticks(rotation=10)

plt.show()
